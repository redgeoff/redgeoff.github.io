<!doctype html><html xmlns=http://www.w3.org/1999/xhtml xml:lang=en-us lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=generator content="Hugo 0.115.3"><title>Running MosaicML's MPT-7B, a ChatGPT Competitor, on AWS SageMaker &#183; redgeoff</title><meta name=description content="In this blog post, Iâ€™m going to take you step-by-step through the process of running MosaicMLâ€™s ChatGPT competitor, MPT-7B, on your own AWS SageMaker instance."><script async src="https://www.googletagmanager.com/gtag/js?id=G-ZMV9CF3YF6"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-ZMV9CF3YF6")</script><meta itemprop=name content="Running MosaicML's MPT-7B, a ChatGPT Competitor, on AWS SageMaker"><meta itemprop=description content="In this blog post, Iâ€™m going to take you step-by-step through the process of running MosaicMLâ€™s ChatGPT competitor, MPT-7B, on your own AWS SageMaker instance."><meta itemprop=datePublished content="2023-07-18T08:48:01-07:00"><meta itemprop=dateModified content="2023-07-18T08:48:01-07:00"><meta itemprop=wordCount content="1656"><meta itemprop=image content="https://redgeoff.com/posts/mpt-7b-on-aws-sagemaker/mpt-7b-sagemaker.png"><meta itemprop=keywords content="chatgpt,aws,mosaicml,mpt-7b,mpt-30b,openai,ai,gpt4,llm,"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://redgeoff.com/posts/mpt-7b-on-aws-sagemaker/mpt-7b-sagemaker.png"><meta name=twitter:title content="Running MosaicML's MPT-7B, a ChatGPT Competitor, on AWS SageMaker"><meta name=twitter:description content="In this blog post, Iâ€™m going to take you step-by-step through the process of running MosaicMLâ€™s ChatGPT competitor, MPT-7B, on your own AWS SageMaker instance."><meta property="og:title" content="Running MosaicML's MPT-7B, a ChatGPT Competitor, on AWS SageMaker"><meta property="og:description" content="In this blog post, Iâ€™m going to take you step-by-step through the process of running MosaicMLâ€™s ChatGPT competitor, MPT-7B, on your own AWS SageMaker instance."><meta property="og:type" content="article"><meta property="og:url" content="https://redgeoff.com/posts/mpt-7b-on-aws-sagemaker/"><meta property="og:image" content="https://redgeoff.com/posts/mpt-7b-on-aws-sagemaker/mpt-7b-sagemaker.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-07-18T08:48:01-07:00"><meta property="article:modified_time" content="2023-07-18T08:48:01-07:00"><script type=application/ld+json>{"@context":"https://schema.org","@graph":[{"@type":"Person","@id":"https://redgeoff.com/#author","name":"Geoff Cox","image":{"@type":"ImageObject","url":"https://www.gravatar.com/avatar/d1488b20cd7d55657e0b69f7d779c942?s=400&d=mp"},"description":" "},{"@type":"WebSite","@id":"https://redgeoff.com/#website","url":"https://redgeoff.com/","name":"redgeoff","description":" ","publisher":{"@id":"https://redgeoff.com/#author"},"inLanguage":"en-us"},{"@type":"WebPage","@id":"https://redgeoff.com/posts/mpt-7b-on-aws-sagemaker/#webpage","url":"https://redgeoff.com/posts/mpt-7b-on-aws-sagemaker/","name":"Running MosaicML's MPT-7B, a ChatGPT Competitor, on AWS SageMaker","isPartOf":{"@id":"https://redgeoff.com/#website"},"about":{"@id":"https://redgeoff.com/#author"},"datePublished":"2023-07-18T08:48:01-07:00","dateModified":"2023-07-18T08:48:01-07:00","description":"In this blog post, Iâ€™m going to take you step-by-step through the process of running MosaicMLâ€™s ChatGPT competitor, MPT-7B, on your own AWS SageMaker instance.","inLanguage":"en-us","potentialAction":[{"@type":"ReadAction","target":["https://redgeoff.com/posts/mpt-7b-on-aws-sagemaker/"]}]},{"@type":"Article","isPartOf":{"@id":"https://redgeoff.com/posts/mpt-7b-on-aws-sagemaker/#webpage"},"mainEntityOfPage":{"@id":"https://redgeoff.com/posts/mpt-7b-on-aws-sagemaker/#webpage"},"headline":"Running MosaicML's MPT-7B, a ChatGPT Competitor, on AWS SageMaker","image":["https://redgeoff.com/posts/mpt-7b-on-aws-sagemaker/mpt-7b-sagemaker.png"],"datePublished":"2023-07-18T08:48:01-07:00","dateModified":"2023-07-18T08:48:01-07:00","publisher":{"@id":"https://redgeoff.com/#author"},"keywords":["chatgpt","aws","mosaicml","mpt-7b","mpt-30b","openai","ai","gpt4","llm"],"articleSection":["Programming"],"inLanguage":"en-us","author":{"@type":"Person","name":null},"potentialAction":[{"@type":"CommentAction","name":"Comment","target":["https://redgeoff.com/posts/mpt-7b-on-aws-sagemaker/#comments"]}]}]}</script><link type=text/css rel=stylesheet href=/css/print.css media=print><link type=text/css rel=stylesheet href=/css/poole.css><link type=text/css rel=stylesheet href=/css/hyde.css><style type=text/css>.sidebar{background-color:#fc2803}.read-more-link a{border-color:#fc2803}.read-more-link a:hover{background-color:#fc2803}.pagination li a{color:#fc2803;border:1px solid #fc2803}.pagination li.active a{background-color:#fc2803}.pagination li a:hover{background-color:#fc2803;opacity:.75}footer a,.content a,.related-posts li a:hover{color:#fc2803}</style><link type=text/css rel=stylesheet href=/css/blog.css><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700&display=swap"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin=anonymous><link rel=apple-touch-icon-precomposed sizes=144x144 href=/apple-touch-icon-144-precomposed.png><link rel="shortcut icon" href=/favicon.png></head><body><aside class=sidebar><div class=container><div class=sidebar-about><div class=author-image><a href=https://redgeoff.com/><img src="https://www.gravatar.com/avatar/d1488b20cd7d55657e0b69f7d779c942?s=200&d=mp" class="img-circle img-headshot center" alt=Gravatar></a></div><h1>redgeoff</h1><p class=lead></p></div><nav><ul class=sidebar-nav><li><a href=https://redgeoff.com/>Home</a></li><li><a href=/posts/>Posts</a></li><li><a href=/publications/>Publications</a></li><li><a href=/about/>About</a></li><li><a href=/music/>Music</a></li></ul></nav><section class=social-icons><a href=https://github.com/redgeoff rel=me title=GitHub target=_blank><i class="fab fa-github" aria-hidden=true></i></a>
<a href=https://www.linkedin.com/in/geoffrey-cox rel=me title=Linkedin target=_blank><i class="fab fa-linkedin" aria-hidden=true></i></a>
<a href=https://medium.com/@redgeoff rel=me title=Medium target=_blank><i class="fab fa-medium" aria-hidden=true></i></a>
<a href="https://stackoverflow.com/users/2831606/redgeoff?tab=profile" rel=me title=StackOverflow target=_blank><i class="fab fa-stack-overflow" aria-hidden=true></i></a>
<a href=https://twitter.com/coxgeoffrey rel=me title=Twitter target=_blank><i class="fab fa-twitter" aria-hidden=true></i></a></section></div></aside><main class="content container"><div class=post><h1 class=title>Running MosaicML's MPT-7B, a ChatGPT Competitor, on AWS SageMaker</h1><div class=post-date><time datetime=2023-07-18T08:48:01-0700>Jul 18, 2023</time> <span class=readtime>&#183; 8 min read</span></div><div><p>In this blog post, Iâ€™m going to take you step-by-step through the process of running MosaicMLâ€™s ChatGPT competitor, MPT-7B, on your own AWS SageMaker instance.</p><figure><img src=/posts/mpt-7b-on-aws-sagemaker/mpt-7b-sagemaker.png alt="MPT-7B and AWS SageMaker"><figcaption><p>MPT-7B and AWS SageMaker. Image credit: <a href=https://www.mosaicml.com/blog/mpt-30b>MosaicML</a> and Author</p></figcaption></figure><p>Are you excited about the capabilities of ChatGPT, but have concerns about exposing your sensitive data to OpenAI? Luckily, there are alternatives that you can run on your own infrastructure. One such alternative is MosaicML&rsquo;s MPT-7b, a competitor to ChatGPT, which we&rsquo;ll explore in this blog post.</p><h3 id=introduction-to-mosaicml-and-mpt-7b>Introduction to MosaicML and MPT-7B</h3><p>MosaicML, recently <a href=https://www.reuters.com/markets/deals/databricks-strikes-13-bln-deal-generative-ai-startup-mosaicml-2023-06-26>acquired by Databricks for $1.3 billion</a>, has been making waves in the ML community with their MPT-7B model, a supposed competitor to ChatGPT. Despite its promise, running this model can be daunting due to sparse documentation and its heavy resource requirements. However, one can run MPT-7B on AWS SageMaker in a Jupyter notebook, an environment that is beginner-friendly and highly flexible for rapid iteration. This setup allows you to test the modelâ€™s feasibility and hardware requirements before deciding to move into production.</p><h3 id=running-mpt-7b-on-aws-sagemaker>Running MPT-7B on AWS SageMaker</h3><p>Running MPT-7B in a Jupyter notebook on AWS SageMaker provides several benefits. Not only can you pay just for what you use and turn it off when you&rsquo;re done, but the ability to easily rerun portions of your code without having to reload the model saves time during iterative development. But do beware! If you forget to stop your notebook instances, the charges can quickly add up.</p><p>While this method is relatively convenient, there are some considerations you must take into account. Firstly, loading the model can take up to 20 minutes even on a high-performance GPU, making this process somewhat time-consuming. Also, the cost is a factor to consider, as the running cost is at least $4 per hour. You&rsquo;ll need to run MPT-7B on at least a <em>p3.2xlarge</em> instance; anything smaller does not seem feasible. If you opt for EC2 instead of SageMaker, you&rsquo;ll have to ask AWS for permission to use a <em>p3.2xlarge</em> instance.</p><p>In the next sections Iâ€™ll take you step-by-step through how to run the MPT-7B model in your very own SageMaker jupyter notebook:</p><h4 id=step-1---open-the-sagemaker-console>Step 1 - Open the SageMaker Console</h4><p>Fire up the AWS Console and search for SageMaker:</p><figure><img src=/posts/mpt-7b-on-aws-sagemaker/sage-maker.png alt="Search for SageMaker"><figcaption><p>Search for SageMaker. Image credit: Author</p></figcaption></figure><h4 id=step-2---create-a-notebook-instance>Step 2 - Create a notebook instance</h4><p>From the left-side menu, select <em>Notebook->Notebook instances</em>:</p><figure><img src=/posts/mpt-7b-on-aws-sagemaker/notebook-instances.png alt="Notebook instances"><figcaption><p>Notebook instances. Image credit: Author</p></figcaption></figure><p>Click the <em>Create notebook instance</em> button:</p><figure><img src=/posts/mpt-7b-on-aws-sagemaker/create-notebook-instance.png alt="Create notebook instance"><figcaption><p>Create notebook instance. Image credit: Author</p></figcaption></figure><p>Specify an instance name. Choose the instance type <em>m1.p3.2xlarge</em>. Unfortunately, it seems that an instance as powerful as <em>m1.p3.2xlarge</em> is required, or else your instance may run out of memory or take an excessive amount of time to respond to even the simplest questions. However, please note that this instance will cost approximately $4/hr, so it&rsquo;s important to monitor your usage carefully.</p><figure><img src=/posts/mpt-7b-on-aws-sagemaker/create-instance-2.png alt="Specify notebook instance details"><figcaption><p>Specify notebook instance details. Image credit: Author</p></figcaption></figure><p>Create a new IAM role:</p><figure><img src=/posts/mpt-7b-on-aws-sagemaker/create-role.png alt="Create a new role"><figcaption><p>Create a new role. Image credit: Author</p></figcaption></figure><p>If your test environment doesnâ€™t have any particularly sensitive data in it then you can grant access to <em>Any S3 bucket</em>. Otherwise, youâ€™ll need to be more explicit.</p><figure><img src=/posts/mpt-7b-on-aws-sagemaker/create-role-s3.png alt="Specify S3 access"><figcaption><p>Specify S3 access. Image credit: Author</p></figcaption></figure><p>Click the <em>Create notebook instance</em> button:</p><figure><img src=/posts/mpt-7b-on-aws-sagemaker/create-instance-3.png alt="Create notebook instance"><figcaption><p>Create notebook instance. Image credit: Author</p></figcaption></figure><p>The notebook will then be in a <em>Pending</em> status. This will likely last for about 10 minutes:</p><figure><img src=/posts/mpt-7b-on-aws-sagemaker/notebook-pending.png alt="Pending notebook instance"><figcaption><p>Pending notebook instance. Image credit: Author</p></figcaption></figure><p>In the meantime, weâ€™ll download a notebook so that we can upload it after the AWS SageMaker instance has finished provisioning.</p><h4 id=step-3---download-the-notebook>Step 3 - Download the notebook</h4><p>Head on over to the notebook at <a href=https://colab.research.google.com/drive/1kJr2LHHLKYkbnNutVYEkt2vrYsbO38aw>MPT-7B on AWS SageMaker.ipynb</a> and download it:</p><figure><img src=/posts/mpt-7b-on-aws-sagemaker/colab-notebook.png alt="The notebook on Google Colab"><figcaption><p>The notebook on Google Colab. Image credit: Author</p></figcaption></figure><figure><img src=/posts/mpt-7b-on-aws-sagemaker/download-colab-notebook.png alt="Download the notebook"><figcaption><p>Download the notebook. Image credit: Author</p></figcaption></figure><p>In this notebook youâ€™ll notice two main code blocks. The first block loads the MPT-7B tokenizer and model:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> torch <span style=color:#f92672>import</span> cuda, bfloat16
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> AutoTokenizer, AutoModelForCausalLM, AutoConfig
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>device <span style=color:#f92672>=</span> <span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;cuda:</span><span style=color:#e6db74>{</span>cuda<span style=color:#f92672>.</span>current_device()<span style=color:#e6db74>}</span><span style=color:#e6db74>&#39;</span> <span style=color:#66d9ef>if</span> cuda<span style=color:#f92672>.</span>is_available() <span style=color:#66d9ef>else</span> <span style=color:#e6db74>&#39;cpu&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>tokenizer <span style=color:#f92672>=</span> AutoTokenizer<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#34;mosaicml/mpt-7b-chat&#34;</span>,
</span></span><span style=display:flex><span>                                          trust_remote_code<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>config<span style=color:#f92672>=</span>{<span style=color:#e6db74>&#34;init_device&#34;</span>: <span style=color:#e6db74>&#34;meta&#34;</span>}
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> AutoModelForCausalLM<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#34;mosaicml/mpt-7b-chat&#34;</span>,
</span></span><span style=display:flex><span>                                             trust_remote_code<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>                                             config<span style=color:#f92672>=</span>config,
</span></span><span style=display:flex><span>                                             torch_dtype<span style=color:#f92672>=</span>bfloat16)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;device=</span><span style=color:#e6db74>{</span>device<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#39;model loaded&#39;</span>)
</span></span></code></pre></div><p>The tokenizer is used to encode the question sent to the model and decode the response from the model. Additionally, we obtain the device specification for our GPU so that we can configure the model to utilize it later:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> time
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> IPython.display <span style=color:#f92672>import</span> Markdown
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> StoppingCriteria, StoppingCriteriaList
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># mtp-7b is trained to add &#34;&lt;|endoftext|&gt;&#34; at the end of generations</span>
</span></span><span style=display:flex><span>stop_token_ids <span style=color:#f92672>=</span> [tokenizer<span style=color:#f92672>.</span>eos_token_id]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Define custom stopping criteria object.</span>
</span></span><span style=display:flex><span><span style=color:#75715e># Source: https://github.com/pinecone-io/examples/blob/master/generation/llm-field-guide/mpt-7b/mpt-7b-huggingface-langchain.ipynb</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>StopOnTokens</span>(StoppingCriteria):
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> __call__(self, input_ids: torch<span style=color:#f92672>.</span>LongTensor,scores: torch<span style=color:#f92672>.</span>FloatTensor,
</span></span><span style=display:flex><span>             <span style=color:#f92672>**</span>kwargs) <span style=color:#f92672>-&gt;</span> bool:
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>for</span> stop_id <span style=color:#f92672>in</span> stop_token_ids:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> input_ids[<span style=color:#ae81ff>0</span>][<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>] <span style=color:#f92672>==</span> stop_id:
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>return</span> <span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>stopping_criteria <span style=color:#f92672>=</span> StoppingCriteriaList([StopOnTokens()])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>ask_question</span>(question, max_length<span style=color:#f92672>=</span><span style=color:#ae81ff>100</span>):
</span></span><span style=display:flex><span>  start_time <span style=color:#f92672>=</span> time<span style=color:#f92672>.</span>time()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#75715e># Encode the question</span>
</span></span><span style=display:flex><span>  input_ids <span style=color:#f92672>=</span> tokenizer<span style=color:#f92672>.</span>encode(question, return_tensors<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;pt&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#75715e># Use the GPU</span>
</span></span><span style=display:flex><span>  input_ids <span style=color:#f92672>=</span> input_ids<span style=color:#f92672>.</span>to(device)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#75715e># Generate a response</span>
</span></span><span style=display:flex><span>  output <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>generate(
</span></span><span style=display:flex><span>    input_ids,
</span></span><span style=display:flex><span>    max_new_tokens<span style=color:#f92672>=</span>max_length,
</span></span><span style=display:flex><span>    temperature<span style=color:#f92672>=</span><span style=color:#ae81ff>0.9</span>,
</span></span><span style=display:flex><span>    stopping_criteria<span style=color:#f92672>=</span>stopping_criteria
</span></span><span style=display:flex><span>  )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#75715e># Decode the response</span>
</span></span><span style=display:flex><span>  response <span style=color:#f92672>=</span> tokenizer<span style=color:#f92672>.</span>decode(output[:, input_ids<span style=color:#f92672>.</span>shape[<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>]:][<span style=color:#ae81ff>0</span>],
</span></span><span style=display:flex><span>                              skip_special_tokens<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  end_time <span style=color:#f92672>=</span> time<span style=color:#f92672>.</span>time()
</span></span><span style=display:flex><span>  duration <span style=color:#f92672>=</span> end_time <span style=color:#f92672>-</span> start_time
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  display(Markdown(response))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  print(<span style=color:#e6db74>&#34;Function duration:&#34;</span>, duration, <span style=color:#e6db74>&#34;seconds&#34;</span>)
</span></span></code></pre></div><p>Note the use of <code>stopping_critera</code>, which is needed or else the model will just start babbling on, even after it has answered our question.</p><p>See <a href=https://huggingface.co/transformers/v4.12.5/main_classes/model.html#transformers.generation_utils.GenerationMixin.generate>model generate parameters</a> if you want to explore the different options.</p><p>Now, letâ€™s upload this notebook to SageMaker.</p><h4 id=step-4---upload-the-notebook>Step 4 - Upload the notebook</h4><p>Hopefully by this time your SageMaker notebook instance has finished being provisioned. When it has, click the <em>Open Jupyter</em> link:</p><figure><img src=/posts/mpt-7b-on-aws-sagemaker/created-notebook-instances.png alt="Open Jupyter"><figcaption><p>Open Jupyter. Image credit: Author</p></figcaption></figure><p>Then, click the <em>Upload</em> button in the top-right corner of your screen and select the notebook that you just downloaded:</p><figure><img src=/posts/mpt-7b-on-aws-sagemaker/upload-notebook.png alt="Upload the notebook"><figcaption><p>Upload the notebook. Image credit: Author</p></figcaption></figure><p>Set the kernel to <em>conda_python3</em>:</p><figure><img src=/posts/mpt-7b-on-aws-sagemaker/set-kernel.png alt="Set the kernel"><figcaption><p>Set the kernel. Image credit: Author</p></figcaption></figure><h4 id=step-5---run-the-notebook>Step 5 - Run the notebook</h4><p>Select <em>Cell -> Run All</em>:</p><figure><img src=/posts/mpt-7b-on-aws-sagemaker/run-all.png alt="Run all cells"><figcaption><p>Run all cells. Image credit: Author</p></figcaption></figure><p>An hourglass logo will then appear in the browser tab:</p><figure><img src=/posts/mpt-7b-on-aws-sagemaker/wait-for-notebook.png alt="Wait for notebook"><figcaption><p>Wait for notebook. Image credit: Author</p></figcaption></figure><p>Youâ€™ll then need to wait about 10 minutes for the model to be downloaded:</p><figure><img src=/posts/mpt-7b-on-aws-sagemaker/notebook-downloads-model.png alt="Download model"><figcaption><p>Download model. Image credit: Author</p></figcaption></figure><p>After it runs, youâ€™ll see the answer to the question <em>Explain to me the difference between nuclear fission and fusion</em>:</p><figure><img src=/posts/mpt-7b-on-aws-sagemaker/fission-vs-fusion.png alt="Explain to me the difference between nuclear fission and fusion"><figcaption><p>Explain to me the difference between nuclear fission and fusion. Image credit: Author</p></figcaption></figure><p>Since the model and tokenizer have already been loaded above, you can simply modify the <em>ask_question</em> code block and click the <em>Run</em> button to ask any other questions. This will save you from spending 10 minutes each time you want to test a new question.</p><figure><img src=/posts/mpt-7b-on-aws-sagemaker/capital-of-france.png alt="What is the capital of France?"><figcaption><p>What is the capital of France?. Image credit: Author</p></figcaption></figure><h4 id=step-6---stop-the-notebook>Step 6 - Stop the notebook</h4><p>As soon as you have finished testing the model, youâ€™ll want to head back to your list of notebook instances and stop it. If you donâ€™t, $4/hr will add up very quickly ðŸ’¸</p><figure><img src=/posts/mpt-7b-on-aws-sagemaker/stop-notebook.png alt="Stop the notebook"><figcaption><p>Stop the notebook. Image credit: Author</p></figcaption></figure><h3 id=performance-comparison>Performance Comparison</h3><p>In terms of performance, my preliminary tests suggest that MPT-7B&rsquo;s results might not be as good as ChatGPTâ€™s. It does a decent job answering questions like <em>What is the capital of France?</em>, <em>Explain to me the difference between nuclear fission and fusion</em>, and <em>Write python code that converts a csv into pdf</em>. But for questions like_What is the capital of Belize?_ it fails pretty horribly:</p><figure><img src=/posts/mpt-7b-on-aws-sagemaker/capital-of-belize.png alt="What is the capital of Belize?"><figcaption><p>What is the capital of Belize? Image credit: Author</p></figcaption></figure><p>I am currently collecting more data and will conduct a comprehensive comparative analysis in a follow-up blog post. In that post, I will compare the question and answer performance of MPT-7B, MPT-30B, Falcon-40b, and ChatGPT using actual conversation history.</p><h3 id=from-testing-to-production>From Testing to Production</h3><p>Once you&rsquo;re ready to transition from testing to production, SageMaker offers an additional benefit - the ability to create an endpoint for your model. With SageMaker, you can auto-scale based on demand to the endpoint, optimizing your resources.</p><h3 id=additional-tips>Additional Tips</h3><p>Be mindful that it&rsquo;s easy for your process to get forked while running in a Jupyter notebook and run out of memory. If this happens, simply shut down the kernel and run all the commands again.</p><p>If you&rsquo;re curious about running this model on a platform other than AWS, Google Colab Pro is another viable option at $9/month. However, based on our testing, we found that we exhausted the provided credits within just a few hours. ðŸ˜³</p><p>Another challenge you may face is the inability to utilize the <a href=https://huggingface.co/mosaicml/mpt-7b#how-to-use>Triton optimization</a> on SageMaker due to a CUDA version incompatibility. Unfortunately, AWS&rsquo;s current P3 instances do not include a recent CUDA version. Therefore, if you wish to utilize the Triton optimization, you will need to create an EC2 container with command line access. However, it&rsquo;s important to note that you will also require special permission from AWS Support to run an instance with 8 VCPUs. In a future post, I will provide a detailed guide on how to integrate Triton and utilize a more cost-effective GPU cloud provider, such as <a href=lambdalabs.com>Lambda Labs</a>.</p><h3 id=final-thoughts>Final Thoughts</h3><p>While MosaicML&rsquo;s MPT-7B offers a viable alternative to OpenAI&rsquo;s ChatGPT, it presents its own set of challenges. Running the model can be time-consuming, expensive, and the available documentation is lacking. However, the ability to keep the model in-house and protect your data from being exposed to OpenAI can be compelling for certain use cases.</p><p>SageMaker offers great convenience for quickly testing the model and provides the flexibility to transition to production when you&rsquo;re ready. Whether you&rsquo;re just starting with MPT-7B or have been using it for a while, we hope this guide has provided valuable insights.</p><p>Stay tuned for our next blog post, where we&rsquo;ll delve deeper into the performance comparisons between MPT-7B, MPT-30B, Falcon-40b, and ChatGPT.</p><p>See the following links if you are interested in learning more about <a href=https://www.mosaicml.com/blog/mpt-7b>MPT-7B</a> or its larger variant, <a href=https://www.mosaicml.com/blog/mpt-30b>MPT-30B</a>.</p><p>And remember, whether you&rsquo;re working with ChatGPT or MPT-7B, the key is to ensure your use case is served without compromising data privacy and cost-effectiveness. Happy tinkering!</p><h3 id=want-a-turn-key-solution-for-using-mpt-or-chatgpt-with-your-data>Want a Turn-Key Solution for Using MPT or ChatGPT with Your Data?</h3><p>At <a href="https://mindfuldataai.com?utm_source=redgeoff_com&amp;utm_campaign=mpt-7b">MindfulDataAI.com</a>, we offer ChatGPT for businesses. If you&rsquo;re interested in leveraging ChatGPT, MPT, or other models with your company&rsquo;s data, please get in touch with us.</p></div><div><ul class=tags><li><a href=https://redgeoff.com/tags/chatgpt/ class=tag-link>chatgpt</a></li><li><a href=https://redgeoff.com/tags/aws/ class=tag-link>aws</a></li><li><a href=https://redgeoff.com/tags/mosaicml/ class=tag-link>mosaicml</a></li><li><a href=https://redgeoff.com/tags/mpt-7b/ class=tag-link>mpt-7b</a></li><li><a href=https://redgeoff.com/tags/mpt-30b/ class=tag-link>mpt-30b</a></li><li><a href=https://redgeoff.com/tags/openai/ class=tag-link>openai</a></li><li><a href=https://redgeoff.com/tags/ai/ class=tag-link>ai</a></li><li><a href=https://redgeoff.com/tags/gpt4/ class=tag-link>gpt4</a></li><li><a href=https://redgeoff.com/tags/llm/ class=tag-link>llm</a></li></ul></div><div class=share-buttons><a class=twitter-share-button href=# title="Share on Twitter" data-url=https://redgeoff.com/posts/mpt-7b-on-aws-sagemaker/ data-text="Running MosaicML's MPT-7B, a ChatGPT Competitor, on AWS SageMaker"><i class="fab fa-twitter"></i></a>
<a class=linkedin-share-button href=# title="Share on LinkedIn" data-url=https://redgeoff.com/posts/mpt-7b-on-aws-sagemaker/ data-text="Running MosaicML's MPT-7B, a ChatGPT Competitor, on AWS SageMaker"><i class="fab fa-linkedin-in"></i></a>
<a class=facebook-share-button href=# title="Share on Facebook" data-url=https://redgeoff.com/posts/mpt-7b-on-aws-sagemaker/ data-text="Running MosaicML's MPT-7B, a ChatGPT Competitor, on AWS SageMaker"><i class="fab fa-facebook"></i></a>
<a class=telegram-share-button href=# title="Share on Telegram" data-url=https://redgeoff.com/posts/mpt-7b-on-aws-sagemaker/ data-text="Running MosaicML's MPT-7B, a ChatGPT Competitor, on AWS SageMaker"><i class="fab fa-telegram"></i></a>
<a class=pinterest-share-button href=# title="Share on Pinterest" data-url=https://redgeoff.com/posts/mpt-7b-on-aws-sagemaker/ data-text="Running MosaicML's MPT-7B, a ChatGPT Competitor, on AWS SageMaker"><i class="fab fa-pinterest"></i></a></div></div></main><footer><div><p>&copy; Geoff Cox 2023</p></div></footer><script src=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/js/all.min.js integrity="sha256-MAgcygDRahs+F/Nk5Vz387whB4kSK9NXlDN3w58LLq0=" crossorigin=anonymous></script>
<script src=/js/jquery.min.js></script>
<script src=/js/soho.js></script></body></html>